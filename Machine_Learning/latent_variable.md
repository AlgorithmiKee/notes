---
title: "Latent Variable Models"
author: "Ke Zhang"
date: "2025"
fontsize: 12pt
---

# Latent Variable Models

[toc]

$$
\DeclareMathOperator*{\argmax}{arg\max}
\DeclareMathOperator*{\argmin}{arg\min}
$$

## Basic Terms

A ***latent variable model*** is given by:

$$
\begin{align}
p_\theta(x,z)
\end{align}
$$

where

* $x\in\mathbb R^d$: observable variable. e.g. An image of a doge
* $z\in\mathbb R^\ell$: latent variable. e.g. High-level abstract description of a doge
* $\theta\in\mathbb R^p$: parameter of the joint distribution of $x$ and $z$

Remarks:

* Typically, $d \gg \ell$, as the latent variable $z$ provides a compressed representation of the high-dimensional observation $x$.
* One example of latent variable model is the Gaussian mixure model (see notes *GMM*)

A latent variable model can be expressed as a **generative model**:

$$
\begin{align}
p_\theta(x,z) = p_\theta(z) \cdot p_\theta(x \mid z)
\end{align}
$$

Remarks:

* In practice, both $p_\theta(z)$ and $p_\theta(x \mid z)$ are chosen to be "simple" distributions.  
  e.g. In GMM, $p_\theta(z)$ is categorical and $p_\theta(x \mid z)$ is Gaussian.
* The observation is generated by first sampling $z \sim p_\theta(z)$, and then sampling $x \sim p_\theta(x \mid z)$.  
  e.g. To generated a picture of a doge, we first sample the latent variable $z$ (which might describe the style and posture), then sample a picture $x$ from $z$.
* In ***variational autoencoder (VAEs)***, $p(z)$ is often chosen to be a standard multivariate gaussian and thus not parameterized.

### Central Tasks

For a **known** and **fixed** paraemter $\theta$, we distinguish between those tasks:

* **Generation**: sample $x \sim p_\theta(x \mid z)$ for a fixed $z$.  
  e.g. In GMM, generate a data point from a specified cluster.
* **MAP estimation** (of latent variable $z$): compute the mode of $p_\theta(z \mid x)$ for a fixed $x$.  
  e.g. In GMM, hard-clusteirn a data point, i.e. compute the most probable cluster which $x$ belongs to.
* **Inference** (of latent variable $z$): compute the full posterior $p_\theta(z \mid x)$ for a fixed $x$.  
  e.g. In GMM, soft-clustering a data point, i.e. compute the probability that $x$ belongs to each cluster.

For unknown paraemter $\theta$, we would like to solve MLE for parameter $\theta$. This can be done by

* **Expectation Maximization (EM)** algorithm: for simpler latent variables models like GMM.
* Training a **VAE**: for more complex latent variable models.

Generation is strightforward because we explicitly model $p_\theta(z)$ and $p_\theta(x \mid z)$.

MAP estimation is often tractable because the mode of the posterior is also the mode of the joint:

$$
\begin{align}
\argmax_z p_\theta(z \mid x)
&= \argmax_z p_\theta(z \mid x) \cdot p_\theta(x) \nonumber \\
&= \argmax_z p_\theta(x, z)
\end{align}
$$

Inference is generally difficult even though we assume that $\theta$ is known.

$$
\begin{align}
p_\theta(z \mid x) = \frac{p_\theta(x, z)}{p_\theta(x)} = \frac{p_\theta(x, z)}{\int p_\theta(x, z)\,dz}
\end{align}
$$

The integral in the denominator is often intractable. Therefore, we often use ***variational inference*** to approximate the true posterior.

## Variational Inference

TODO
