---
title: "Latent Variable Models"
author: "Ke Zhang"
date: "2025"
fontsize: 12pt
---

# Latent Variable Models

[toc]

$$
\DeclareMathOperator*{\argmax}{arg\max}
\DeclareMathOperator*{\argmin}{arg\min}
$$

## Overview

A ***latent variable model*** is given by:

$$
\begin{align}
p_\theta(\mathbf{x},\mathbf{z}) = p_\theta(\mathbf{z}) \cdot p_\theta(\mathbf{x} \vert \mathbf{z})
\end{align}
$$

where

* $\mathbf{x}\in\mathbb R^d$: observable variable. (e.g. an image of a doge)
* $\mathbf{z}\in\mathbb R^\ell$ (or discrete): latent variable. (e.g. an abstract description such as posture or style of the image)
* $\theta\in\mathbb R^p$: parameter of the joint distribution of $\mathbf{x}$ and $\mathbf{z}$

Remarks:

* Typically, $d \gg \ell$, as the latent variable $\mathbf{z}$ provides a compressed representation of the high-dimensional observation $\mathbf{x}$.
* The observation $\mathbf{x}$ is generated by first sampling $\mathbf{z} \sim p_\theta(\mathbf{z})$, and then sampling $\mathbf{x} \sim p_\theta(\mathbf{x} \vert \mathbf{z})$

For example, to generate a picture of a doge, we first sample $\mathbf{z}$ (encoding e.g. the style and posture), and then sample a picture $\mathbf{x}$ conditioned on $\mathbf{z}$.

Although both $p_\theta(\mathbf{z})$ and $p_\theta(\mathbf{x}\mid\mathbf{z})$ are typically simple distributions, the induced marginal distribution $p_\theta(\mathbf{x})$ can be highly complex. This makes latent variable models very flexible for modeling complex data patterns.

$$
\begin{align}
p_\theta(\mathbf{x}) = \int_{\mathbf{z}} p_\theta(\mathbf{x},\mathbf{z}) \:\mathrm d\mathbf{z}
\end{align}
$$

Remarks:

* The marginal distribution $p_\theta(\mathbf{x})$ is generally intractable to compute unless for simple latent variable models like [GMM](#gmm).
* In [VAE](#vae), $p(\mathbf{z})$ is often chosen to be a standard multivariate gaussian and thus not parameterized.

**Example**: Consider $z \in \mathbb R$ and $\mathbf{x} \in \mathbb R^2$ such that
$$
\begin{align*}
\underbrace{
  \begin{bmatrix}
    x_1 \\ x_2
  \end{bmatrix}
}_{\mathbf{x}}
&=
\underbrace{
  \begin{bmatrix}
    \sin(z) \cdot \log\left(|z| + 2 \right) \\
    \cos(z) \cdot \log\left(|z| + 1 \right) \\
  \end{bmatrix}
}_{\boldsymbol{\mu}(z)}
+
\underbrace{
  \begin{bmatrix}
    \varepsilon_1 \\ \varepsilon_2
  \end{bmatrix}
}_{\boldsymbol{\varepsilon}}
\end{align*}
$$

where $\boldsymbol{\varepsilon}$ is independent of $z$, and

$$
\begin{align*}
z \sim \mathcal N(0,1), \quad
\varepsilon \sim \mathcal N(\mathbf{0}, \sigma^2\mathbf{I})
\end{align*}
$$

Equivalently, the latent variable model is

$$
p(\mathbf{x}, z) =
\underbrace{\mathcal N(z ; 0,1)}_{p(z)} \cdot
\underbrace{\mathcal N(\mathbf{x} ; \boldsymbol{\mu}(z), \sigma^2\mathbf{I})}_{p(\mathbf{x} \mid z)}
$$

Even though both $p(z)$ and $p(\mathbf{x} \mid z)$ are simple distributions (Gaussian), the marginal distribution $p(\mathbf{x})$ can be more complex. At $\sigma = 0.1$, the marginal distribution $p(\mathbf{x})$ appears to be heart-shaped. This illustrates how a latent variable model can generate a complex distribution from simple components. The key is that the mapping $\boldsymbol{\mu}(z)$ can be highly nonlinear. Extending this idea to higher dimensions gives rise to the decoder network in VAEs.

<img src="./figs/latent_var_heart_x.pdf" alt="Heart Shaped Data" style="zoom:100%;" />

## Typical Latent Variable Models

### GMM

In GMM, the latent variable model is given by

$$
\begin{align}
z
&\sim \mathrm{Cat}(\pi_1,\dots,\pi_K)
\\
\mathbf{x} \mid z
&\sim \mathcal N(\boldsymbol{\mu}_z, \boldsymbol{\Sigma}_z)
\\
p(\mathbf{x}, z)
&= \pi_z \cdot \mathcal N(\mathbf{x}; \boldsymbol{\mu}_z, \boldsymbol{\Sigma}_z)
\end{align}
$$

where $\mathbf{x} \in \mathbb R^d$ and $z \in \{1,\dots,K\}$.

Parameters of GMM:

* $\pi_1,\dots,\pi_K \ge 0$: the cluster probabilities s.t. $\sum_{k=1}^K \pi_k = 1$.
* $\boldsymbol{\mu}_1, \dots, \boldsymbol{\mu}_K \in \mathbb R^d$: cluster centers.
* $\boldsymbol{\Sigma}_1, \dots, \boldsymbol{\Sigma}_K \in \mathbb R^{d \times d}$: cluster covariance matrices.

The data is generated in two steps:

1. Sample $z$ from a categorical distribution with probabilities $\pi_1,\dots,\pi_K$.
1. Sample $\mathbf{x}$ from the Gaussian distribution $\mathcal N(\boldsymbol{\mu}_z, \boldsymbol{\Sigma}_z)$.

The resulting marginal distribution over $\mathbf{x}$ consists multiple clusters of Gaussians
$$
\begin{align}
p(\mathbf{x})
&= \sum_{z=1}^K \pi_z \cdot \mathcal N(\mathbf{x}; \boldsymbol{\mu}_z, \boldsymbol{\Sigma}_z)
\end{align}
$$

### Probabilistic PCA

Probabilistic PCA (PPCA) is a latent variable model, given by

$$
\begin{align}
\mathbf{z}
&\sim \mathcal N(\mathbf{0}, \mathbf{I}) \\
\mathbf{x} \mid \mathbf{z}
&\sim 
\mathcal N(\mathbf{W}\mathbf{z} + \mathbf{b}, \sigma^2 \mathbf{I}) \\
p(\mathbf{x}, \mathbf{z})
&= \mathcal N(\mathbf{z} ; \mathbf{0}, \mathbf{I}) \cdot
  \mathcal N(\mathbf{x} ; \mathbf{W}\mathbf{z} + \mathbf{b}, \sigma^2 \mathbf{I}).
\end{align}
$$

Parameters of PPCA:
* $\mathbf{W} \in \mathbb R^{d \times \ell}$ is a loading matrix (analogous to PCA projection directions),
* $\mathbf{b} \in \mathbb R^d$ is the mean of the data,
* $\sigma^2$ is the isotropic noise variance.

Equivalently, we can write PPCA as
$$
\begin{align}
\mathbf{z}
&\sim \mathcal N(\mathbf{0}, \mathbf{I}) \nonumber \\
\mathbf{x}
&= \mathbf{W}\mathbf{z} + \mathbf{b} + \boldsymbol{\varepsilon},\quad
\boldsymbol{\varepsilon} \sim \mathcal N(\mathbf{0}, \sigma^2 \mathbf{I}),\:
\boldsymbol{\varepsilon} \perp \mathbf{z}
\end{align}
$$
Since $\mathbf{x}$ is an affine transform of $\mathbf{z}$, the joint distribution is also Gaussian (proof omitted)
$$
\begin{bmatrix}
    \mathbf{z} \\ \mathbf{x}
\end{bmatrix}
\sim \mathcal N \left(
    \begin{bmatrix}
        \mathbf{0} \\ \mathbf{b}
    \end{bmatrix},
    \begin{bmatrix}
        \mathbf{I} & \mathbf{W}^\top \\
        \mathbf{W} & \mathbf{W}\mathbf{W}^\top + \sigma^2\mathbf{I} \\
    \end{bmatrix}
\right)
$$

The marginal distribution over $\mathbf{x}$ is also Gaussian (closeness of Gaussian):

$$
p(\mathbf{x})
= \mathcal N\bigl(\mathbf{x} ; \mathbf{b}, \mathbf{W}\mathbf{W}^\top + \sigma^2 \mathbf{I} \bigr).
$$

### Decoder Network in VAEs

In a VAE, the decoder network generates $\mathbf{x}$ from $\mathbf{z}$. Typically,

* $\mathbf{z}$ is sampled from the standard Gaussian distribution.
* The conditional mean $\mathbb E[\mathbf{x} \mid \mathbf{z}]$ is computed by the decoder net, whose weights are denoted by $\theta$.
* The conditional covariance is often assumed to be $\mathbb{V}[\mathbf{x}\mid\mathbf{z}] = \sigma^2 \mathbf{I}$, where $\sigma$ is treated as a fixed hyperparameter. In more general VAEs, the decoder also outputs $\sigma_\theta^2(\mathbf{z})$, making the variance depend on $\mathbf{z}$.

Assuming $\sigma$ is a hyperparameter, the resulting latent variable model is

$$
\begin{align}
\mathbf{z}
&\sim \mathcal N(\mathbf{0},\mathbf{I})
\\
\mathbf{x} \mid \mathbf{z}
&\sim \mathcal N(\boldsymbol{\mu}_{\theta}(\mathbf{z}), \sigma^2\mathbf{I})
\\
p(\mathbf{x}, \mathbf{z})
&= \mathcal N(\mathbf{z} ; \mathbf{0},\mathbf{I}) \cdot
   \mathcal N(\mathbf{x} ; \boldsymbol{\mu}_{\theta}(\mathbf{z}), \sigma^2\mathbf{I})
\end{align}
$$

Assuming $\sigma$ is produced by the decoder net, the resulting latent variable model is

$$
\begin{align}
\mathbf{z}
&\sim \mathcal N(\mathbf{0},\mathbf{I}) \nonumber
\\
\mathbf{x} \mid \mathbf{z}
&\sim \mathcal N(\boldsymbol{\mu}_{\theta}(\mathbf{z}), \sigma_\theta^2(\mathbf{z})\mathbf{I})
\\
p(\mathbf{x}, \mathbf{z})
&= \mathcal N(\mathbf{z} ; \mathbf{0},\mathbf{I}) \cdot
   \mathcal N(\mathbf{x} ; \boldsymbol{\mu}_{\theta}(\mathbf{z}), \sigma_\theta^2(\mathbf{z})\mathbf{I})
\end{align}
$$

Remarks:

* Due to the expressiveness of neural nets, both $\boldsymbol{\mu}_{\theta}(\mathbf{z})$ and $\sigma_{\theta}(\mathbf{z})$ can be highly nonlinear in $\mathbf{z}$. Thus, the joint distribution $p(\mathbf{x}, \mathbf{z})$ is generally non Gaussian.
* The decoder net can be viewed as nonlinear extension of PPCA .
* The marginal distribution $p(\mathbf{x})$ has no closed-form expression.

## Central Tasks

For a **known** and **fixed** parameter $\theta$, we distinguish between following tasks:

* **Generation**: sample $\mathbf{x} \sim p_\theta(\mathbf{x} \vert \mathbf{z})$ for a fixed $\mathbf{z}$.  
  e.g. In GMM, generate a data point from a specified cluster.
* **MAP estimation** (of latent variable $\mathbf{z}$): compute the mode of $p_\theta(\mathbf{z} \vert \mathbf{x})$ for a fixed $\mathbf{x}$.  
  e.g. In GMM, hard-clustering a data point, i.e. compute the most probable cluster which $\mathbf{x}$ belongs to.
* **Inference** (of latent variable $\mathbf{z}$): compute/approximate the full posterior $p_\theta(\mathbf{z} \vert \mathbf{x})$ for a fixed $\mathbf{x}$.  
  e.g. In GMM, soft-clustering a data point, i.e. compute the probability that $\mathbf{x}$ belongs to each cluster.

For **unknown** parameter $\theta$, we would like estimate (MLE) it from data. This can be done by

* **Expectation Maximization (EM)** algorithm: for simpler latent variables models like GMM.
* Training a **VAE**: for more complex latent variable models.

Generation is straightforward because we explicitly model $p_\theta(\mathbf{z})$ and $p_\theta(\mathbf{x} \vert \mathbf{z})$.

MAP estimation is often tractable because the mode of the posterior is also the mode of the joint:

$$
\begin{align}
\argmax_{\mathbf{z}} p_\theta(\mathbf{z} \vert \mathbf{x})
&= \argmax_{\mathbf{z}} p_\theta(\mathbf{z} \vert \mathbf{x}) \cdot p_\theta(\mathbf{x}) \nonumber \\
&= \argmax_{\mathbf{z}} p_\theta(\mathbf{x}, \mathbf{z})
\end{align}
$$

Inference is generally difficult even though we assume that $\theta$ is known.

$$
\begin{align}
p_\theta(\mathbf{z} \vert \mathbf{x}) = \frac{p_\theta(\mathbf{x}, \mathbf{z})}{p_\theta(\mathbf{x})} = \frac{p_\theta(\mathbf{x}, \mathbf{z})}{\int p_\theta(\mathbf{x}, \mathbf{z})\,d\mathbf{z}}
\end{align}
$$

The integral in the denominator is often intractable. Therefore, we often use ***variational inference (VI)*** to approximate the true posterior.

Parameter estimation is also challenging since we only observe $\mathbf{x}$, not $\mathbf{z}$.

## Variational Inference

### Problem Formulation

Given:

* Parameter $\theta$ of the latent variable model.
* Dataset: $D = \{ \mathbf{x}^{(1)}, \dots, \mathbf{x}^{(n)} \} \stackrel{\text{iid}}{\sim} p^*$.

Goal: approximate the posterior $p_\theta(\mathbf{z} \vert \mathbf{x}^{(i)})$ using a tractable variational distribution $q_\phi(\mathbf{z} \vert \mathbf{x})$.

Remarks:

* $\phi$ is called ***variational parameter***. The set of all possible $q_\phi(\mathbf{z} \vert \mathbf{x})$ by varying $\phi$ is called ***variational family***.
* Variational inference transforms the intractable inference problem to a tractable optimization problem.


### Evidence Lower Bound

The evidence lower bound (ELBO) for each $\mathbf{x}^{(i)} \in D$ is defined by

$$
\begin{align}
\mathcal L_{\theta, \phi}(\mathbf{x}^{(i)})
\triangleq \mathbb E_{\mathbf{z} \sim q_\phi(\mathbf{z} \vert \mathbf{x}^{(i)})} \left[ \log\frac{p_\theta(\mathbf{x}^{(i)},\mathbf{z})}{q_\phi(\mathbf{z} \vert \mathbf{x}^{(i)})} \right]
\end{align}
$$

The ELBO is a lower bound because

$$
\begin{align}
\log p_\theta(\mathbf{x}^{(i)})
&= \mathcal L_{\theta, \phi}(\mathbf{x}^{(i)}) + D_\text{KL}(q_\phi(\mathbf{z} \vert \mathbf{x}^{(i)}) \parallel p_\theta(\mathbf{z} \vert \mathbf{x}^{(i)})) \\
&\ge \mathcal L_{\theta, \phi}(\mathbf{x}^{(i)})
\end{align}
$$

where the inequality in the 2nd line becomes equality iff $q_\phi(\mathbf{z} \vert \mathbf{x}^{(i)}) = p_\theta(\mathbf{z} \vert \mathbf{x}^{(i)})$ a.e.

*Proof*:

$$
\begin{align*}
\mathcal L_{\theta, \phi}(\mathbf{x}^{(i)})
&= \mathbb E_{\mathbf{z} \sim q_\phi(\mathbf{z} \vert \mathbf{x}^{(i)})} \left[ \log\frac{p_\theta(\mathbf{z} \vert \mathbf{x}^{(i)}) \cdot p_\theta(\mathbf{x}^{(i)})}{q_\phi(\mathbf{z} \vert \mathbf{x}^{(i)})} \right] \\
&= \mathbb E_{\mathbf{z} \sim q_\phi(\mathbf{z} \vert \mathbf{x}^{(i)})} \left[ \log\frac{p_\theta(\mathbf{z} \vert \mathbf{x}^{(i)})}{q_\phi(\mathbf{z} \vert \mathbf{x}^{(i)})} + \log p_\theta(\mathbf{x}^{(i)}) \right] \\
&= \underbrace{\mathbb E_{\mathbf{z} \sim q_\phi(\mathbf{z} \vert \mathbf{x}^{(i)})} \left[ \log\frac{p_\theta(\mathbf{z} \vert \mathbf{x}^{(i)})}{q_\phi(\mathbf{z} \vert \mathbf{x}^{(i)})} \right]}_{- D_\text{KL}(q_\phi(\mathbf{z} \vert \mathbf{x}^{(i)}) \parallel p_\theta(\mathbf{z} \vert \mathbf{x}^{(i)}))} + \log p_\theta(\mathbf{x}^{(i)})
\end{align*}
$$

Rearranging the terms, we conclude the equality in the 1st line. The inequality in the 2nd line follows from properties of KL divergence. $\:\blacksquare$

Hence, for each $\mathbf{x}^{(i)} \in D$, minimizing the KL divergence is equivalent to maximizing the ELBO

$$
\begin{align}
\max_\phi \mathcal L_{\theta, \phi}(\mathbf{x}^{(i)})
\end{align}
$$

### Classical VI

Classical VI optimzes $\phi$ for each $\mathbf{x}^{(i)} \in D$ individually using a tractable variational family. Formally, classical VI solves $n$ optimization problems:

$$
\begin{align}
\phi^{(i)*} = \argmax_\phi \mathcal L_{\theta, \phi}(\mathbf{x}^{(i)}),
\quad i = 1,\dots,n
\end{align}
$$

Remarks:

* Typical tractable variational family: mean-field Gaussian.
* If the variational family is conjugate, $\phi^{(i)*}$ has closed-form solution. (not detailed here)
* Limitation: $\phi$ is optimized separately for each data point and has to be recomputed for new observations. This motivates us to move to amortized VI.

### Amortized VI

Key idea of amortized VI: Let $\phi$ be shared by all $\mathbf{x} \in \mathbb R^d$ (not just $\mathbf{x} \in D$!). We maximize the dataset level ELBO which is defined as the sum of sample level ELBO.

$$
\begin{align}
\mathcal L_{\theta, \phi}(D) &= \sum_{i=1}^n \mathcal L_{\theta, \phi}(\mathbf{x}^{(i)}) \\
\phi^*
= \argmax_\phi \mathcal L_{\theta, \phi}(D) &= \argmax_\phi \sum_{i=1}^n \mathcal L_{\theta, \phi}(\mathbf{x}^{(i)}) \\
\end{align}
$$

A typical choice of variational family is Gaussian:

$$
\begin{align}
q_\phi(\mathbf{z} \vert \mathbf{x}) = \mathcal N(\mathbf{z} ; \mu_\phi(\mathbf{x}), \Sigma_\phi(\mathbf{x}))
\end{align}
$$

where $\mu_\phi(\mathbf{x}), \Sigma_\phi(\mathbf{x})$ are functions of $\mathbf{x}$, parameterized by $\phi$. In VAEs, those functions are represented by neural nets (whose weights are $\phi$) and thus can be arbitrarily complex. In mean-field settings, the covariance matrix $\Sigma_\phi(\mathbf{x})$ is diagonal, reducing the number of variational parameters.

The dataset level ELBO and its gradient could not be computed directly because

1. For large dataset, the dataset level ELBO consists a huge amount of sample level ELBO.
1. The gradient and the expectation in ELBO are both taken w.r.t. $\phi$. Solution: reparameterization trick.

The 1st issue can be addressed by SGD (sample a mini batch from $D$). The 2nd issue is addressed by reparameterization trick. (see notes *gradient approximation*) for Gaussian variational family.

The complete algorithm is summarized in notes *variational inference*.
