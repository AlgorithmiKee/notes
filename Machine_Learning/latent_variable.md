---
title: "Latent Variable Models"
author: "Ke Zhang"
date: "2025"
fontsize: 12pt
---

# Latent Variable Models

[toc]

$$
\DeclareMathOperator*{\argmax}{arg\max}
\DeclareMathOperator*{\argmin}{arg\min}
$$

## Basic Terms

We have seen Gaussian mixture model (see notes GMM) before. Now, we look at the general latent variable models.

A ***latent variable model*** is given by:

$$
\begin{align}
p_\theta(\mathbf{x},\mathbf{z}) = p_\theta(\mathbf{z}) \cdot p_\theta(\mathbf{x} \vert \mathbf{z})
\end{align}
$$

where

* $\mathbf{x}\in\mathbb R^d$: observable variable. e.g. An image of a doge
* $\mathbf{z}\in\mathbb R^\ell$ (or discrete): latent variable. e.g. High-level abstract description of a doge
* $\theta\in\mathbb R^p$: parameter of the joint distribution of $\mathbf{x}$ and $\mathbf{z}$

Remarks:

* Typically, $d \gg \ell$, as the latent variable $\mathbf{z}$ provides a compressed representation of the high-dimensional observation $\mathbf{x}$.
* The observation $\mathbf{x}$ is generated by first sampling $\mathbf{z} \sim p_\theta(\mathbf{z})$, and then sampling $\mathbf{x} \sim p_\theta(\mathbf{x} \vert \mathbf{z})$.  
  e.g. To generated a picture of a doge, we first sample the latent variable $\mathbf{z}$ (which might describe the style and posture), then sample a picture $\mathbf{x}$ from $\mathbf{z}$.

In practice, both $p_\theta(\mathbf{z})$ and $p_\theta(\mathbf{x} \vert \mathbf{z})$ are chosen to be "simple" distributions.  The marginal distribution $p_\theta(\mathbf{x})$ can however be highly complex, leading to high flexibility to model complex distributions.

$$
\begin{align}
p_\theta(\mathbf{x}) = \int_z p_\theta(\mathbf{x},\mathbf{z}) \:\mathrm dz
\end{align}
$$

Remarks:

* The marginal distribution $p_\theta(\mathbf{x})$ is generally intractable to compute unless for simple latent variable models like GMM.
* In ***variational autoencoder (VAEs)***, $p(\mathbf{z})$ is often chosen to be a standard multivariate gaussian and thus not parameterized.

|                      | GMM | VAE |
|--------------------- | --- | --- |
| $p_\theta(\mathbf{z})$        | categorical | standard Gaussian |
| $p_\theta(\mathbf{x} \vert \mathbf{z})$ | Gaussian | Gaussian |
| $p_\theta(\mathbf{x})$        | Gaussian mixutre | Infinite mixture (more expressive) |

### Central Tasks

For a **known** and **fixed** parameter $\theta$, we distinguish between following tasks:

* **Generation**: sample $\mathbf{x} \sim p_\theta(\mathbf{x} \vert \mathbf{z})$ for a fixed $\mathbf{z}$.  
  e.g. In GMM, generate a data point from a specified cluster.
* **MAP estimation** (of latent variable $\mathbf{z}$): compute the mode of $p_\theta(\mathbf{z} \vert \mathbf{x})$ for a fixed $\mathbf{x}$.  
  e.g. In GMM, hard-clustering a data point, i.e. compute the most probable cluster which $\mathbf{x}$ belongs to.
* **Inference** (of latent variable $\mathbf{z}$): compute/approximate the full posterior $p_\theta(\mathbf{z} \vert \mathbf{x})$ for a fixed $\mathbf{x}$.  
  e.g. In GMM, soft-clustering a data point, i.e. compute the probability that $\mathbf{x}$ belongs to each cluster.

For **unknown** parameter $\theta$, we would like estimate (MLE) it from data. This can be done by

* **Expectation Maximization (EM)** algorithm: for simpler latent variables models like GMM.
* Training a **VAE**: for more complex latent variable models.

Generation is straightforward because we explicitly model $p_\theta(\mathbf{z})$ and $p_\theta(\mathbf{x} \vert \mathbf{z})$.

MAP estimation is often tractable because the mode of the posterior is also the mode of the joint:

$$
\begin{align}
\argmax_z p_\theta(\mathbf{z} \vert \mathbf{x})
&= \argmax_z p_\theta(\mathbf{z} \vert \mathbf{x}) \cdot p_\theta(\mathbf{x}) \nonumber \\
&= \argmax_z p_\theta(\mathbf{x}, \mathbf{z})
\end{align}
$$

Inference is generally difficult even though we assume that $\theta$ is known.

$$
\begin{align}
p_\theta(\mathbf{z} \vert \mathbf{x}) = \frac{p_\theta(\mathbf{x}, \mathbf{z})}{p_\theta(\mathbf{x})} = \frac{p_\theta(\mathbf{x}, \mathbf{z})}{\int p_\theta(\mathbf{x}, \mathbf{z})\,dz}
\end{align}
$$

The integral in the denominator is often intractable. Therefore, we often use ***variational inference (VI)*** to approximate the true posterior.

Parameter estimation is also challenging since we only observe $\mathbf{x}$, not $\mathbf{z}$.

## Variational Inference

### Problem Formulation

Given:

* Parameter $\theta$ of the latent variable model
* Dataset: $D = \{ \mathbf{x}^{(1)}, \dots, \mathbf{x}^{(n)} \}$ (TODO: question: sampled from which distribution? $p_\theta(\mathbf{x})$ or unknown ground truth $p^*(\mathbf{x})$? )

Goal: compute the posterior $p_\theta(\mathbf{z} \vert \mathbf{x}^{(i)})$ for each $i=1,\dots,n$

### Classical VI

TODO

### Amortized VI

Key idea of amortized VI: Instead of computing $p_\theta(\mathbf{z} \vert \mathbf{x})$ directly, we use another tractable posterior $q_\phi(\mathbf{z} \vert \mathbf{x})$ to approximate $p_\theta(\mathbf{z} \vert \mathbf{x})$. For each $\mathbf{x} \in D$, the loss of $q_\phi(\mathbf{z} \vert \mathbf{x})$ is given by the KL divergence

$$
\begin{align}
D_\text{KL}(q_\phi(\mathbf{z} \vert \mathbf{x}) \parallel p_\theta(\mathbf{z} \vert \mathbf{x}))
&= \mathbb E_{\mathbf{z} \sim q_\phi(\mathbf{z} \vert \mathbf{x})} \left[ \log\frac{q_\phi(\mathbf{z} \vert \mathbf{x})}{p_\theta(\mathbf{z} \vert \mathbf{x})} \right]
\end{align}
$$

Remarks:

* $\phi$ is called ***variational parameter***. $q_\phi(\mathbf{z} \vert \mathbf{x})$  is called ***variational distribution***.
* In amortized VI, the same variational parameter $\phi$ is shared by all $\mathbf{x} \in \mathbb R^d$.

This reformulates the inference problem into an optimization problem

$$
\begin{align}
\min_\phi D_\text{KL}(q_\phi(\mathbf{z} \vert \mathbf{x}) \parallel p_\theta(\mathbf{z} \vert \mathbf{x}))
\end{align}
$$

Remarks:

* The KL divergence requires knowledge of $p_\theta(\mathbf{z} \vert \mathbf{x})$, which is what we want to approximate in the first place. Later, we will make this optimization problem tractable by introducing [evidence lower bound](#evidence-lower-bound).

#### Example: Variational Gaussian

Suppose $\mathbf{z}\in\mathbb R^\ell$. We use multivariate Gaussian as the variational distribution:

$$
\begin{align}
q_\phi(\mathbf{z} \vert \mathbf{x}) = \mathcal N(\mathbf{z} ; \mu_\phi(\mathbf{x}), \Sigma_\phi(\mathbf{x}))
\end{align}
$$

where $\mu_\phi(\mathbf{x}), \Sigma_\phi(\mathbf{x})$ are functions of $\mathbf{x}$, parameterized by $\phi$. In VAEs, those functions are represented by neural nets (whose weights are $\phi$) and thus can be arbitrarily complex. In mean field VI, the covariance matrix $\Sigma_\phi(\mathbf{x})$ is diagonal, reducing the number of variational parameters.

TODO: pytorch code for variational gaussian

### Evidence Lower Bound

The evidence lower bound (ELBO) for each $\mathbf{x} \in D$ is defined by

$$
\begin{align}
\mathcal L_{\theta, \phi}(\mathbf{x})
\triangleq \mathbb E_{\mathbf{z} \sim q_\phi(\mathbf{z} \vert \mathbf{x})} \left[ \log\frac{p_\theta(\mathbf{x},\mathbf{z})}{q_\phi(\mathbf{z} \vert \mathbf{x})} \right]
\end{align}
$$

The ELBO is a lower bound because

$$
\begin{align}
\log p_\theta(\mathbf{x})
&= \mathcal L_{\theta, \phi}(\mathbf{x}) + D_\text{KL}(q_\phi(\mathbf{z} \vert \mathbf{x}) \parallel p_\theta(\mathbf{z} \vert \mathbf{x})) \\
&\ge \mathcal L_{\theta, \phi}(\mathbf{x})
\end{align}
$$

where the inequality in the 2nd line becomes equality iff $q_\phi(\mathbf{z} \vert \mathbf{x}) = p_\theta(\mathbf{z} \vert \mathbf{x})$ a.e.

*Proof*:

$$
\begin{align*}
\mathcal L_{\theta, \phi}(\mathbf{x})
&= \mathbb E_{\mathbf{z} \sim q_\phi(\mathbf{z} \vert \mathbf{x})} \left[ \log\frac{p_\theta(\mathbf{z} \vert \mathbf{x}) \cdot p_\theta(\mathbf{x})}{q_\phi(\mathbf{z} \vert \mathbf{x})} \right] \\
&= \mathbb E_{\mathbf{z} \sim q_\phi(\mathbf{z} \vert \mathbf{x})} \left[ \log\frac{p_\theta(\mathbf{z} \vert \mathbf{x})}{q_\phi(\mathbf{z} \vert \mathbf{x})} + \log p_\theta(\mathbf{x}) \right] \\
&= \underbrace{\mathbb E_{\mathbf{z} \sim q_\phi(\mathbf{z} \vert \mathbf{x})} \left[ \log\frac{p_\theta(\mathbf{z} \vert \mathbf{x})}{q_\phi(\mathbf{z} \vert \mathbf{x})} \right]}_{- D_\text{KL}(q_\phi(\mathbf{z} \vert \mathbf{x}) \parallel p_\theta(\mathbf{z} \vert \mathbf{x}))} + \log p_\theta(\mathbf{x})
\end{align*}
$$

Rearranging the terms, we conconclude the equality in the 1st line. The inequality in the 2nd line follows from properties of KL divergence. $\:\blacksquare$

Therefore, minimizing the KL divergence is equivalent to maximizing the ELBO

$$
\begin{align}
\max_\phi \mathcal L_{\theta, \phi}(\mathbf{x})
\end{align}
$$

### Reparameterization Trick

We apply SGD to maximize the ELBO w.r.t. $\phi$

$$
\begin{align}
\text{random. draw } \mathbf{x} \in D \\
\phi \leftarrow \phi + \eta_t \nabla_\phi \mathcal L_{\theta, \phi}(\mathbf{x})
\end{align}
$$

However, the gradient of the ELBO cannot be computed directly:

$$
\begin{align}
\nabla_\phi \mathcal L_{\theta, \phi}(\mathbf{x})
&= \nabla_\phi \mathbb E_{\mathbf{z} \sim q_\phi(\mathbf{z} \vert \mathbf{x})} \left[ \log\frac{p_\theta(\mathbf{x},\mathbf{z})}{q_\phi(\mathbf{z} \vert \mathbf{x})} \right]
\end{align}
$$

We cannot move the gradient operator $\nabla_\phi$ inside the expectation because the distribution $q_\phi(\mathbf{z} \vert \mathbf{x})$ depends on $\phi$. This problem can be tackled by **reparameterization trick**. (see notes *gradient approximation*)

We find a function $g$ and a reference random variable $\epsilon$ s.t.

1. The reference density $p(\epsilon)$ does not depend on $\phi$.
1. $\mathbf{z} = g_{\mathbf{x}, \phi}(\epsilon)$ has the distribution $q_\phi(\mathbf{z} \vert \mathbf{x})$.
1. $g_{\mathbf{x}, \phi}$ is differentiable w.r.t. $\phi$.

After we apply the reparameterizaiton trick, both the ELBO and its gradient are expressed in the form $\mathbb E_{\epsilon \sim p(\epsilon)}[\cdot]$. Hence, they can be estimated by Monte Carlo sampling.

$$
\begin{align}
\mathcal L_{\theta, \phi}(\mathbf{x})
&= \mathbb E_{\epsilon \sim p(\epsilon)} \left[ \left.
  \log\frac{p_\theta(\mathbf{x},\mathbf{z})}{q_\phi(\mathbf{z} \vert \mathbf{x})}
\right|_{\mathbf{z}=g_{\mathbf{x}, \phi}(\epsilon)} \right]
\\
\nabla_\phi \mathcal L_{\theta, \phi}(\mathbf{x})
&= \mathbb E_{\epsilon \sim p(\epsilon)} \left[ \left.
  \nabla_\phi \log\frac{p_\theta(\mathbf{x},\mathbf{z})}{q_\phi(\mathbf{z} \vert \mathbf{x})}
\right|_{\mathbf{z}=g_{\mathbf{x}, \phi}(\epsilon)} \right]
\end{align}
$$

For example, the variational Gaussian

$$
\begin{align*}
q_\phi(\mathbf{z} \vert \mathbf{x}) = \mathcal N(\mathbf{z} ; \mu_\phi(\mathbf{x}), \Sigma_\phi(\mathbf{x}))
\end{align*}
$$

can be reparameterized as

$$
\begin{align}
\mathbf{z} = \mu_\phi(\mathbf{x}) + L_\phi(\mathbf{x}) \cdot \epsilon, \quad \epsilon \sim\mathcal N(0, I)
\end{align}
$$

where $L_\phi(\mathbf{x})$ is the Chekovskey factor of the covariance matrix $\Sigma_\phi(\mathbf{x})$.

In special case where $\mathbf{z}\in\mathbb R$, the reparameterization simplifies to

$$
\begin{align}
\mathbf{z} = \mu_\phi(\mathbf{x}) + \sigma_\phi(\mathbf{x}) \cdot \epsilon, \quad \epsilon \sim\mathcal N(0, 1)
\end{align}
$$

### The Complete VI Algorithm
